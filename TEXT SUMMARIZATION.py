# -*- coding: utf-8 -*-
"""Copy of NLP Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mjW03aFU0OkJKd_XpHzYINdnNQzAJb9Q
"""

pip install transformers

from transformers import pipeline
from bs4 import BeautifulSoup
import requests

summarizer = pipeline("summarization")

pip install beautifulsoup4

pip install lxml

import bs4 as bs
import urllib.request
import re

pip install nltk

import nltk
nltk.download('punkt')

import nltk
nltk.download('stopwords')

pip install -q transformers

pip install -q youtube_transcript_api

from transformers import pipeline
from youtube_transcript_api import YouTubeTranscriptApi

def switch():
    print('Welcome to text summarization')
    print('1. input as blog')
    print('2. input as wikipedia')
    print('3. input as youtube video')
    print('4. input as document')
    
    option = int(input(" Give the input type for text summarization(1 or 2 or 3 or 4): "))


    if option == 1:
      

      URL = input("Which URL would you want me to summarize: ")
      r = requests.get(URL)
      soup = BeautifulSoup(r.text, 'html.parser')
      results = soup.find_all(['h1', 'p'])
      text = [result.text for result in results]
      ARTICLE = ' '.join(text)
      print("text in blog:")
      print( ARTICLE)
      print("length of the blog:")
      print(len(ARTICLE))
      max_chunk = 500
      ARTICLE = ARTICLE.replace('.', '.')
      ARTICLE = ARTICLE.replace('?', '?')
      ARTICLE = ARTICLE.replace('!', '!')
      sentences = ARTICLE.split(' ')
      current_chunk = 0 
      chunks = []
      for sentence in sentences:
        if len(chunks) == current_chunk + 1: 
          if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:
            chunks[current_chunk].extend(sentence.split(' '))
          else:
            current_chunk += 1
            chunks.append(sentence.split(' '))
        else:
            
            chunks.append(sentence.split(' '))
  
      for chunk_id in range(len(chunks)):
        chunks[chunk_id] = ' '.join(chunks[chunk_id])
    
      
      res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)
      
      ' '.join([summ['summary_text'] for summ in res])
      text = ' '.join([summ['summary_text'] for summ in res])
      with open('blogsummary.txt', 'w') as f:
        f.write(text)
      print("summarized text:")
      print(text)
      print("length of the summarized  text:")
      print(len(text))
      

    elif option == 2:
      userLink = input("Which Wikipedia article would you want me to summarize: ")
      raw_data = urllib.request.urlopen(userLink) 
      document = raw_data.read()
      parsed_document = bs.BeautifulSoup(document,'lxml')
      article_paras = parsed_document.find_all('p')
      scrapped_data = ""

      for para in article_paras:
        scrapped_data += para.text
      print("text in wikipedia:")
      print(scrapped_data)
      print("length of data in wikipedia:")
      print(len(scrapped_data))
      scrapped_data = re.sub(r'\[[0-9]*\]', ' ',  scrapped_data)
      scrapped_data = re.sub(r'\s+', ' ',  scrapped_data)
      formatted_text = re.sub('[^a-zA-Z]', ' ', scrapped_data)
      formatted_text = re.sub(r'\s+', ' ', formatted_text)
      import nltk #if you don't have it, then>> python3 -m pip install nltk
      all_sentences = nltk.sent_tokenize(scrapped_data)

      stopwords = nltk.corpus.stopwords.words('english')

      word_freq = {}
      for word in nltk.word_tokenize(formatted_text):
        if word not in stopwords:
          if word not in word_freq.keys():
            word_freq[word] = 1
          else:
            word_freq[word] += 1
      max_freq = max(word_freq.values())

      for word in word_freq.keys():
        word_freq[word] = (word_freq[word]/max_freq)
      sentence_scores = {}
      for sentence in all_sentences:
        for token in nltk.word_tokenize(sentence.lower()):
          if token in word_freq.keys():
            if len(sentence.split(' ')) <25:
              if sentence not in sentence_scores.keys():
                sentence_scores[sentence] = word_freq[token]
              else:
                sentence_scores[sentence] += word_freq[token]
      import heapq
      selected_sentences= heapq.nlargest(5, sentence_scores, key=sentence_scores.get)

      text_summary = ' '.join(selected_sentences)
      print("summarized text:")
      print(text_summary)
      print("length of the summarized  text:")
      print(len(text_summary))
      
 
    elif option == 3:
      youtube_video = input("Which youtube video would you want me to summarize: ")
      video_id = youtube_video.split("=")[1]
      from IPython.display import YouTubeVideo
      YouTubeVideo(video_id)
      YouTubeTranscriptApi.get_transcript(video_id)
      transcript = YouTubeTranscriptApi.get_transcript(video_id)
      print("text in Youtube video:")
      result = ""
      for i in transcript:
        result += ' ' + i['text']
      print(result)
      print("length of data in Youtube video:")
      print(len(result))
      summarizer = pipeline('summarization')
      num_iters = int(len(result)/1000)
      summarized_text = []
      for i in range(0,num_iters+1):
        start =0
        start= i*1000
        end = (i+1)*1000
        out = summarizer(result[start:end])
        out = out[0]
        out =out['summary_text']
      summarized_text.append(out)
      text=str(summarized_text)
      print("summarized text:")
      print(text)
      print("length of the summarized  text:")
      print(len(text))
    
    elif option == 4:
      filetype =input("enter a filetype:")
      if filetype=="textfile":
        import spacy
        from spacy.lang.en.stop_words import STOP_WORDS
        from string import punctuation
        f=open('/content/education.txt','r',errors='ignore')
        text=f.read()
        stopwords=list(STOP_WORDS)
        nlp=spacy.load('en_core_web_sm')
        doc=nlp(text)
        token=[token.text for token in doc]
        word_frequencies={}
        for word in doc:
          if word.text.lower() not in stopwords:
            if word.text.lower() not in punctuation:
              if word.text not in word_frequencies.keys():
                word_frequencies[word.text]=1
              else:
                word_frequencies[word.text] += 1
        max_frequency=max(word_frequencies.values())
        for word in word_frequencies.keys():
          word_frequencies[word]=word_frequencies[word]/max_frequency
        sentence_tokens=[sent for sent in doc.sents]
        sentence_scores={}
        for sent in sentence_tokens:
          for word in sent:
            if word.text.lower() in word_frequencies.keys():
              if sent not in sentence_scores.keys():
                sentence_scores[sent]=word_frequencies[word.text.lower()]
              else:
                sentence_scores[sent] += word_frequencies[word.text.lower()]
        from heapq import nlargest
        select_length=int(len(sentence_tokens)*0.3)
        summary=nlargest(select_length,sentence_scores,key=sentence_scores.get)
        print(summary)
      else: 
        print("enter the correct file type")
 
    else:
      print("Incorrect option")
 
switch()